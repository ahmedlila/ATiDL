{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np \n",
    "import torchvision.transforms.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Custom dataset class to load original and rotated images\n",
    "class RotationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.all_imgs_paths = []\n",
    "        dir_list = [os.path.join(self.root_dir, directory) for directory in os.listdir(self.root_dir)]\n",
    "        for dir_name in dir_list:\n",
    "            self.all_imgs_paths.extend([os.path.join(dir_name, img_name) for img_name in os.listdir(dir_name)])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.all_imgs_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_image = self.transform(image)\n",
    "        label = 0\n",
    "        return original_image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define transformations\n",
    "def rotate_90(image):\n",
    "    return F.rotate(image, 90)\n",
    "\n",
    "def rotate_180(image):\n",
    "    return F.rotate(image, 180)\n",
    "\n",
    "def rotate_270(image):\n",
    "    return F.rotate(image, 270)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151414fbcc5b4a44a059ed509e8e0df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103b31a88774de2ba5afe48e0228d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5b254d9fcd434798f8746b41709500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7170a321e693471da9f1afbfc3d03adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e64788363d6445da99ab03b0468c80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7615d0e4bb472e8b09d738628430b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset using custom dataset class\n",
    "train_dataset = RotationDataset(root_dir='imagenette2/train', transform=transform)\n",
    "# Apply transformations to create rotated datasets\n",
    "rotated_90_dataset = [(rotate_90(image), 1) for image, label in tqdm(train_dataset)]\n",
    "rotated_180_dataset = [(rotate_180(image), 2) for image, label in tqdm(train_dataset)]\n",
    "rotated_270_dataset = [(rotate_270(image), 3) for image, label in tqdm(train_dataset)]\n",
    "\n",
    "# Concatenate all datasets\n",
    "concatenated_dataset = ConcatDataset([train_dataset, \n",
    "                                      rotated_90_dataset, \n",
    "                                      rotated_180_dataset, \n",
    "                                      rotated_270_dataset])\n",
    "\n",
    "# Create a DataLoader to handle the concatenated dataset\n",
    "dataloader = DataLoader(concatenated_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = RotationDataset(root_dir='imagenette2/val', transform=transform)\n",
    "# Apply transformations to create rotated datasets\n",
    "rotated_90_val_dataset = [(rotate_90(image), 1) for image, label in tqdm(val_dataset)]\n",
    "rotated_180_val_dataset = [(rotate_180(image), 2) for image, label in tqdm(val_dataset)]\n",
    "rotated_270_val_dataset = [(rotate_270(image), 3) for image, label in tqdm(val_dataset)]\n",
    "\n",
    "# Concatenate all datasets\n",
    "concatenated_val_dataset = ConcatDataset([val_dataset, \n",
    "                                      rotated_90_val_dataset, \n",
    "                                      rotated_180_val_dataset, \n",
    "                                      rotated_270_val_dataset])\n",
    "\n",
    "# Create a DataLoader to handle the concatenated dataset\n",
    "val_dataloader = DataLoader(concatenated_val_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.7\n",
      "IPython version      : 8.20.0\n",
      "\n",
      "torch    : 2.3.0\n",
      "lightning: 2.2.4\n",
      "\n",
      "Torch CUDA available? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0100 | Batch 0000/1184 | Loss: 1.3901\n",
      "Epoch: 0001/0100 | Batch 0300/1184 | Loss: 1.0135\n",
      "Epoch: 0001/0100 | Batch 0600/1184 | Loss: 1.1674\n",
      "Epoch: 0001/0100 | Batch 0900/1184 | Loss: 1.4280\n",
      "Epoch: 0001/0100 | Train acc.: 45.08% | Val acc.: 49.36%\n",
      "Epoch: 0002/0100 | Batch 0000/1184 | Loss: 1.1761\n",
      "Epoch: 0002/0100 | Batch 0300/1184 | Loss: 1.2253\n",
      "Epoch: 0002/0100 | Batch 0600/1184 | Loss: 1.2709\n",
      "Epoch: 0002/0100 | Batch 0900/1184 | Loss: 1.0439\n",
      "Epoch: 0002/0100 | Train acc.: 49.00% | Val acc.: 49.88%\n",
      "Epoch: 0003/0100 | Batch 0000/1184 | Loss: 1.1471\n",
      "Epoch: 0003/0100 | Batch 0300/1184 | Loss: 1.3531\n",
      "Epoch: 0003/0100 | Batch 0600/1184 | Loss: 1.1270\n",
      "Epoch: 0003/0100 | Batch 0900/1184 | Loss: 1.0715\n",
      "Epoch: 0003/0100 | Train acc.: 50.52% | Val acc.: 50.97%\n",
      "Epoch: 0004/0100 | Batch 0000/1184 | Loss: 0.9851\n",
      "Epoch: 0004/0100 | Batch 0300/1184 | Loss: 0.9874\n",
      "Epoch: 0004/0100 | Batch 0600/1184 | Loss: 1.1966\n",
      "Epoch: 0004/0100 | Batch 0900/1184 | Loss: 1.1087\n",
      "Epoch: 0004/0100 | Train acc.: 51.31% | Val acc.: 52.17%\n",
      "Epoch: 0005/0100 | Batch 0000/1184 | Loss: 1.0325\n",
      "Epoch: 0005/0100 | Batch 0300/1184 | Loss: 1.1176\n",
      "Epoch: 0005/0100 | Batch 0600/1184 | Loss: 0.9083\n",
      "Epoch: 0005/0100 | Batch 0900/1184 | Loss: 1.0744\n",
      "Epoch: 0005/0100 | Train acc.: 52.78% | Val acc.: 53.12%\n",
      "Epoch: 0006/0100 | Batch 0000/1184 | Loss: 1.1229\n",
      "Epoch: 0006/0100 | Batch 0300/1184 | Loss: 0.9133\n",
      "Epoch: 0006/0100 | Batch 0600/1184 | Loss: 0.9088\n",
      "Epoch: 0006/0100 | Batch 0900/1184 | Loss: 1.0661\n",
      "Epoch: 0006/0100 | Train acc.: 53.58% | Val acc.: 54.55%\n",
      "Epoch: 0007/0100 | Batch 0000/1184 | Loss: 0.9580\n",
      "Epoch: 0007/0100 | Batch 0300/1184 | Loss: 1.0420\n",
      "Epoch: 0007/0100 | Batch 0600/1184 | Loss: 1.1225\n",
      "Epoch: 0007/0100 | Batch 0900/1184 | Loss: 0.9293\n",
      "Epoch: 0007/0100 | Train acc.: 54.28% | Val acc.: 54.83%\n",
      "Epoch: 0008/0100 | Batch 0000/1184 | Loss: 0.9717\n",
      "Epoch: 0008/0100 | Batch 0300/1184 | Loss: 0.6541\n",
      "Epoch: 0008/0100 | Batch 0600/1184 | Loss: 1.1190\n",
      "Epoch: 0008/0100 | Batch 0900/1184 | Loss: 0.9869\n",
      "Epoch: 0008/0100 | Train acc.: 55.25% | Val acc.: 55.90%\n",
      "Epoch: 0009/0100 | Batch 0000/1184 | Loss: 0.8224\n",
      "Epoch: 0009/0100 | Batch 0300/1184 | Loss: 1.0117\n",
      "Epoch: 0009/0100 | Batch 0600/1184 | Loss: 1.1039\n",
      "Epoch: 0009/0100 | Batch 0900/1184 | Loss: 1.3634\n",
      "Epoch: 0009/0100 | Train acc.: 56.02% | Val acc.: 56.06%\n",
      "Epoch: 0010/0100 | Batch 0000/1184 | Loss: 0.9722\n",
      "Epoch: 0010/0100 | Batch 0300/1184 | Loss: 0.9220\n",
      "Epoch: 0010/0100 | Batch 0600/1184 | Loss: 0.8278\n",
      "Epoch: 0010/0100 | Batch 0900/1184 | Loss: 0.7965\n",
      "Epoch: 0010/0100 | Train acc.: 56.77% | Val acc.: 56.57%\n",
      "Epoch: 0011/0100 | Batch 0000/1184 | Loss: 1.0667\n",
      "Epoch: 0011/0100 | Batch 0300/1184 | Loss: 0.9881\n",
      "Epoch: 0011/0100 | Batch 0600/1184 | Loss: 1.0824\n",
      "Epoch: 0011/0100 | Batch 0900/1184 | Loss: 0.9619\n",
      "Epoch: 0011/0100 | Train acc.: 57.58% | Val acc.: 57.62%\n",
      "Epoch: 0012/0100 | Batch 0000/1184 | Loss: 1.1440\n",
      "Epoch: 0012/0100 | Batch 0300/1184 | Loss: 0.9197\n",
      "Epoch: 0012/0100 | Batch 0600/1184 | Loss: 0.9614\n",
      "Epoch: 0012/0100 | Batch 0900/1184 | Loss: 0.9201\n",
      "Epoch: 0012/0100 | Train acc.: 58.24% | Val acc.: 57.81%\n",
      "Epoch: 0013/0100 | Batch 0000/1184 | Loss: 0.7883\n",
      "Epoch: 0013/0100 | Batch 0300/1184 | Loss: 1.0204\n",
      "Epoch: 0013/0100 | Batch 0600/1184 | Loss: 1.0388\n",
      "Epoch: 0013/0100 | Batch 0900/1184 | Loss: 1.1121\n",
      "Epoch: 0013/0100 | Train acc.: 58.89% | Val acc.: 58.76%\n",
      "Epoch: 0014/0100 | Batch 0000/1184 | Loss: 0.9449\n",
      "Epoch: 0014/0100 | Batch 0300/1184 | Loss: 0.9679\n",
      "Epoch: 0014/0100 | Batch 0600/1184 | Loss: 0.9558\n",
      "Epoch: 0014/0100 | Batch 0900/1184 | Loss: 0.6747\n",
      "Epoch: 0014/0100 | Train acc.: 59.36% | Val acc.: 59.81%\n",
      "Epoch: 0015/0100 | Batch 0000/1184 | Loss: 0.9125\n",
      "Epoch: 0015/0100 | Batch 0300/1184 | Loss: 0.9184\n",
      "Epoch: 0015/0100 | Batch 0600/1184 | Loss: 1.0005\n",
      "Epoch: 0015/0100 | Batch 0900/1184 | Loss: 1.0908\n",
      "Epoch: 0015/0100 | Train acc.: 60.03% | Val acc.: 59.27%\n",
      "Epoch: 0016/0100 | Batch 0000/1184 | Loss: 0.9426\n",
      "Epoch: 0016/0100 | Batch 0300/1184 | Loss: 0.8897\n",
      "Epoch: 0016/0100 | Batch 0600/1184 | Loss: 1.0798\n",
      "Epoch: 0016/0100 | Batch 0900/1184 | Loss: 0.9601\n",
      "Epoch: 0016/0100 | Train acc.: 60.45% | Val acc.: 59.60%\n",
      "Epoch: 0017/0100 | Batch 0000/1184 | Loss: 1.0695\n",
      "Epoch: 0017/0100 | Batch 0300/1184 | Loss: 0.9430\n",
      "Epoch: 0017/0100 | Batch 0600/1184 | Loss: 0.6889\n",
      "Epoch: 0017/0100 | Batch 0900/1184 | Loss: 0.9348\n",
      "Epoch: 0017/0100 | Train acc.: 61.20% | Val acc.: 60.46%\n",
      "Epoch: 0018/0100 | Batch 0000/1184 | Loss: 0.8975\n",
      "Epoch: 0018/0100 | Batch 0300/1184 | Loss: 0.8099\n",
      "Epoch: 0018/0100 | Batch 0600/1184 | Loss: 1.2728\n",
      "Epoch: 0018/0100 | Batch 0900/1184 | Loss: 0.7619\n",
      "Epoch: 0018/0100 | Train acc.: 61.32% | Val acc.: 61.36%\n",
      "Epoch: 0019/0100 | Batch 0000/1184 | Loss: 0.9891\n",
      "Epoch: 0019/0100 | Batch 0300/1184 | Loss: 0.7676\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 124\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m#########################################\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m### 3 Finetuning\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m#########################################\u001b[39;00m\n\u001b[0;32m    123\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 124\u001b[0m train(\n\u001b[0;32m    125\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCHS,\n\u001b[0;32m    126\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    127\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m    128\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m    129\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m    130\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[0;32m    131\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    132\u001b[0m )\n\u001b[0;32m    134\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    135\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m end\u001b[38;5;241m-\u001b[39mstart\n",
      "Cell \u001b[1;32mIn[3], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs, model, optimizer, scheduler, train_loader, val_loader, device)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     73\u001b[0m         predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m         train_acc\u001b[38;5;241m.\u001b[39mupdate(predicted_labels, targets)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m### MORE LOGGING\u001b[39;00m\n\u001b[0;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torchmetrics\\metric.py:482\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 482\u001b[0m         update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torchmetrics\\classification\\stat_scores.py:339\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[1;32m--> 339\u001b[0m     _multiclass_stat_scores_tensor_validation(\n\u001b[0;32m    340\u001b[0m         preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[0;32m    341\u001b[0m     )\n\u001b[0;32m    342\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m    343\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[0;32m    344\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[0;32m    345\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torchmetrics\\functional\\classification\\stat_scores.py:317\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[1;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[0;32m    315\u001b[0m check_value \u001b[38;5;241m=\u001b[39m num_classes \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, name \u001b[38;5;129;01min\u001b[39;00m ((target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;241m+\u001b[39m ((preds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m ():  \u001b[38;5;66;03m# noqa: RUF005\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(torch\u001b[38;5;241m.\u001b[39munique(t))\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_unique_values \u001b[38;5;241m>\u001b[39m check_value:\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected more unique values in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` than expected. Expected only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_unique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in `target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torch\\_jit_internal.py:497\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torch\\_jit_internal.py:497\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torch\\functional.py:996\u001b[0m, in \u001b[0;36m_return_output\u001b[1;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[1;32m--> 996\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torch\\functional.py:910\u001b[0m, in \u001b[0;36m_unique_impl\u001b[1;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[0;32m    902\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[0;32m    903\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    904\u001b[0m         dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    907\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[0;32m    908\u001b[0m     )\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_unique2(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m,\n\u001b[0;32m    913\u001b[0m         return_inverse\u001b[38;5;241m=\u001b[39mreturn_inverse,\n\u001b[0;32m    914\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[0;32m    915\u001b[0m     )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchvision import transforms\n",
    "from watermark import watermark\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class PyTorchCNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 7 * 7, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(num_epochs, model, optimizer, scheduler, train_loader, val_loader, device):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4).to(device)\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            ### FORWARD AND BACK PROP\n",
    "            logits = model(features)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            ### UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            ### LOGGING\n",
    "            if not batch_idx % 300:\n",
    "                print(f\"Epoch: {epoch+1:04d}/{num_epochs:04d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {loss:.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                predicted_labels = torch.argmax(logits, 1)\n",
    "                train_acc.update(predicted_labels, targets)\n",
    "\n",
    "        ### MORE LOGGING\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4).to(device)\n",
    "\n",
    "            for (features, targets) in val_loader:\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(features)\n",
    "                predicted_labels = torch.argmax(outputs, 1)\n",
    "                val_acc.update(predicted_labels, targets)\n",
    "\n",
    "            print(f\"Epoch: {epoch+1:04d}/{num_epochs:04d} | Train acc.: {train_acc.compute()*100:.2f}% | Val acc.: {val_acc.compute()*100:.2f}%\")\n",
    "            train_acc.reset(), val_acc.reset()\n",
    "\n",
    "\n",
    "\n",
    "print(watermark(packages=\"torch,lightning\", python=True))\n",
    "print(\"Torch CUDA available?\", torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "L.seed_everything(123)\n",
    "\n",
    "##########################\n",
    "### 1 Loading the Dataset\n",
    "##########################\n",
    "train_loader, val_loader = dataloader, val_dataloader\n",
    "\n",
    "\n",
    "#########################################\n",
    "### 2 Initializing the Model\n",
    "#########################################\n",
    "\n",
    "\n",
    "model = PyTorchCNN(num_classes=4)\n",
    "model.to(device)\n",
    "############################################################\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "num_steps = NUM_EPOCHS * len(train_loader)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "\n",
    "#########################################\n",
    "### 3 Finetuning\n",
    "#########################################\n",
    "\n",
    "start = time.time()\n",
    "train(\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    scheduler=scheduler,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")\n",
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
